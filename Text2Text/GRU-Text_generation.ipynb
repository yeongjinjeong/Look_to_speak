{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d452e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aade11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./example_df2.csv\")\n",
    "df2 = pd.read_csv(\"./input_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250f1e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "4000\n",
      "7000\n",
      "8000\n",
      "8000\n",
      "9000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "20000\n",
      "21000\n",
      "25000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "32000\n",
      "35000\n",
      "35000\n",
      "35000\n",
      "37000\n",
      "39000\n",
      "39000\n",
      "39000\n",
      "39000\n",
      "39000\n",
      "39000\n",
      "40000\n",
      "42000\n",
      "42000\n",
      "42000\n",
      "42000\n",
      "44000\n",
      "45000\n",
      "45000\n",
      "45000\n",
      "46000\n",
      "48000\n",
      "49000\n",
      "51000\n",
      "52000\n",
      "52000\n",
      "52000\n",
      "52000\n",
      "52000\n",
      "52000\n",
      "53000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "60000\n",
      "62000\n",
      "62000\n",
      "62000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "63000\n",
      "64000\n",
      "64000\n",
      "65000\n",
      "65000\n",
      "65000\n",
      "65000\n",
      "65000\n",
      "67000\n",
      "67000\n",
      "67000\n",
      "67000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "69000\n",
      "69000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "72000\n",
      "73000\n",
      "73000\n",
      "73000\n",
      "73000\n",
      "73000\n",
      "74000\n",
      "74000\n",
      "75000\n",
      "75000\n",
      "75000\n",
      "75000\n",
      "75000\n",
      "77000\n",
      "77000\n",
      "78000\n",
      "78000\n",
      "78000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "79000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "80000\n",
      "82000\n",
      "82000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "83000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "84000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "87000\n",
      "87000\n",
      "87000\n",
      "87000\n",
      "87000\n",
      "87000\n",
      "88000\n",
      "88000\n",
      "88000\n",
      "88000\n"
     ]
    }
   ],
   "source": [
    "word_set = []\n",
    "count = 0\n",
    "for j in range(len(df2[\"value\"])):\n",
    "    for i in okt.morphs(df2[\"value\"][j]):\n",
    "        if i not in word_set:\n",
    "            word_set.append(i)\n",
    "            count += 1\n",
    "    if count%1000==0:\n",
    "        print(count)\n",
    "for j in range(len(df2[\"label\"])):\n",
    "    for i in okt.morphs(df2[\"label\"][j]):\n",
    "        if i not in word_set:\n",
    "            word_set.append(i)\n",
    "    if count%1000==0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42cb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "23e5ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse = dict(map(reversed,vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e0c1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCorpus(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_sentence = self.df[\"value\"][idx]\n",
    "        label_sentence = self.df[\"label\"][idx]\n",
    "        \n",
    "        token_ids = [vocab[i] for i in okt.morphs(input_sentence)]\n",
    "        x = torch.zeros(647, dtype=torch.long)\n",
    "        for i in range(len(token_ids)):\n",
    "            x[i] = token_ids[i]\n",
    "            \n",
    "        label_ids = [vocab[i] for i in okt.morphs(label_sentence)]\n",
    "        y = torch.zeros(647, dtype=torch.float32)\n",
    "        for i in range(len(label_ids)):\n",
    "            y[i] = label_ids[i]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c45a8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123778, 35365, 17682)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(df2)*(7/10)), int(len(df2)*(2/10)), int(len(df2)*(1/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6365879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df2.iloc[:1024,:]\n",
    "valid_df = df2.iloc[1024:1024+256,:]\n",
    "test_df = df2.iloc[1280:1280+128,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "38357d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = NewsCorpus(train_df)\n",
    "tr_dataloader = DataLoader(\n",
    "    tr_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "#     num_workers=12,\n",
    ")\n",
    "\n",
    "val_dataset = NewsCorpus(valid_df)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "#     num_workers=5,\n",
    ")\n",
    "\n",
    "ts_dataset = NewsCorpus(test_df)\n",
    "ts_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=False,\n",
    "#     num_workers=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b165fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 647]) torch.Size([1, 647])\n",
      "torch.int64 torch.float32\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(tr_dataloader))\n",
    "print(x.shape, y.shape)\n",
    "print(x.dtype, y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d469184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 647, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "torch.randn(batch_size, 647, 512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "06faf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.wte = nn.Embedding(89162, 647, padding_idx=0)\n",
    "        self.gru1 = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True)  # Additional GRU layer\n",
    "        self.gru3 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.gru4 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(165632, output_size) ## 647 * 256\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = nn.init.xavier_normal_(torch.randn(1, batch_size, self.hidden_size).to(x.device), gain=1.0)\n",
    "        hidden_state0 = h0.cpu().detach().numpy()\n",
    "        wte = self.wte(x)\n",
    "        \n",
    "        output, hidden1 = self.gru1(wte, (h0))\n",
    "        hidden_state1 = hidden1.cpu().detach().numpy()\n",
    "        \n",
    "        output = self.dropout(self.relu(output))\n",
    "        \n",
    "        output, hidden2 = self.gru2(output, hidden1)\n",
    "        hidden_state2 = hidden2.cpu().detach().numpy()\n",
    "        \n",
    "        output = self.dropout(self.relu(output))\n",
    "        \n",
    "        output, hidden3 = self.gru3(output, hidden2)\n",
    "        hidden_state3 = hidden3.cpu().detach().numpy()\n",
    "        \n",
    "        output = self.dropout(self.relu(output))\n",
    "        \n",
    "        output3, hidden4 = self.gru4(output, hidden3)\n",
    "        hidden_state4 = hidden4.cpu().detach().numpy()\n",
    "        \n",
    "        output = self.dropout(self.relu(output3))\n",
    "        \n",
    "        output = self.fc(self.relu(self.flatten(output)))\n",
    "        \n",
    "        return output, [hidden_state0, hidden_state1, hidden_state2, hidden_state3, hidden_state4, output3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "61921c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUModel(x.shape[1], y.shape[1], 256).to(device)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=.1)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.85 ** epoch,\n",
    "#                                         last_epoch=-1,\n",
    "                                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "95005896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (wte): Embedding(89162, 647, padding_idx=0)\n",
       "  (gru1): GRU(647, 256, batch_first=True)\n",
       "  (gru2): GRU(256, 256, batch_first=True)\n",
       "  (gru3): GRU(256, 256, batch_first=True)\n",
       "  (gru4): GRU(256, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=165632, out_features=647, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af0fc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n",
      "torch.Size([128, 647]) torch.Size([128, 647])\n"
     ]
    }
   ],
   "source": [
    "for i, (x,y) in enumerate(tr_dataloader):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70fa6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n",
    "        '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "        '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "        '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', colors, len(colors))\n",
    "\n",
    "def layer_visualization(hidden_states, text):\n",
    "    plt.figure(figsize=(7 * len(hidden_states), 4)) # Adjust the figure size\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        plt.subplot(1, len(hidden_states), layer_idx + 1) # Create a subplot for each layer\n",
    "        plt.title(f'Hidden state values in layer {layer_idx + 1}')\n",
    "        sns.heatmap(hidden_state[0,:20], cmap=cmap)\n",
    "#         print([reverse[int(i)] for i in text[:10,:20]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show() # Call `plt.show()` once after creating all subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "10b3527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstr(s, color='black'):\n",
    "    if s == ' ':\n",
    "        return \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n",
    "    else:\n",
    "        return \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c41ffcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def print_color(t):\n",
    "    display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    z = 1/(1 + np.exp(-x)) \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e18693fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:#000;background-color:#85c2e1>나 </text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(html_print(''.join(cstr('나', color='#85c2e1'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1326ce06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '는', '밥', '을', '먹었습니다']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt.morphs(\"나는 밥을 먹었습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5bcefd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [[\"나\", '#85c2e1'],\n",
    "    [\"는\", '#f42e2e'],\n",
    "    [\"밥\", '#f8a8a8'],\n",
    "    [\"을\", '#baddee'],\n",
    "    [\"먹었습니다\", '#89c4e2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5a892b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<text style=color:#000;background-color:#85c2e1>나 </text><text style=color:#000;background-color:#f42e2e>는 </text><text style=color:#000;background-color:#f8a8a8>밥 </text><text style=color:#000;background-color:#baddee>을 </text><text style=color:#000;background-color:#89c4e2>먹었습니다 </text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_color(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3c3bbcd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Adjusting learning rate of group 0 to 8.5000e-02.\n",
      "Epoch [1/1024], Loss: nan, Time Duration : 101.4830\n",
      "tensor([[369., 453., 454.,   5.,   2.,  18.,  19.,  20., 455., 183.,   5.,   2.,\n",
      "          62., 281., 107., 359.,  19., 456., 457., 318.,  74.,  58., 458.,   5.,\n",
      "         459.,   2.,  16., 460., 461.,   5., 462., 463., 464.,  24.,  86.,  18.,\n",
      "          19.,  20., 465.,  65.,  62., 466., 466., 177.,  72.,  73.,  14.,  13.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
      "       device='cuda:0')\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<CudnnRnnBackward0>)\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         x,y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         print(x.shape, y.shape)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         outputs, hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#         print(outputs.shape)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         print(len(hidden_states))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[153], line 35\u001b[0m, in \u001b[0;36mGRUModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m hidden_state3 \u001b[38;5;241m=\u001b[39m hidden3\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(output))\n\u001b[0;32m---> 35\u001b[0m output3, hidden4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru4\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m hidden_state4 \u001b[38;5;241m=\u001b[39m hidden4\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(output3))\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/nn/modules/rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1002\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "timelist = []\n",
    "\n",
    "batch_size=1\n",
    "model.train()\n",
    "\n",
    "num_epochs = 1024\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_step = 0\n",
    "    start = time()\n",
    "    if epoch%128 == 0:\n",
    "        torch.save(model.state_dict(), \"./Untitled Folder/Text2Text.pt\")\n",
    "    print(epoch)\n",
    "    for i, (x,y) in enumerate(tr_dataloader):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "#         print(x.shape, y.shape)\n",
    "        \n",
    "        outputs, hidden_states = model(x)\n",
    "#         print(outputs.shape)\n",
    "#         print(len(hidden_states))\n",
    "        loss = criterion(outputs, y)\n",
    "#         print(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_step = i\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == y.argmax(dim=1)).sum().item()\n",
    "    scheduler.step()\n",
    "    epoch_loss = total_loss / (batch_size * total_step)\n",
    "    epoch_acc = total_correct / (batch_size * total_step)\n",
    "    end = time()\n",
    "    timelist.append(end-start)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Time Duration : {end - start:.4f}')\n",
    "#     layer_visualization(hidden_states, y)\n",
    "    print(y)\n",
    "    print(hidden_states[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "711cf78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 647])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e733ea7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 256)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "10422d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.29340002,  1.        , -1.        , -1.        ,\n",
       "         -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  0.9962113 ,  1.        ,  1.        ,\n",
       "          1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,\n",
       "          1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "          1.        ,  1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        ,  0.23928717, -1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "          1.        ,  1.        , -1.        , -1.        ,\n",
       "          1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "          0.43182868,  1.        , -1.        ,  1.        ,\n",
       "          1.        ,  1.        , -1.        , -1.        ,\n",
       "         -0.07222224, -1.        , -1.        , -1.        ,\n",
       "          1.        , -1.        ,  1.        ,  1.        ,\n",
       "         -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         -1.        ,  1.        , -0.7241914 , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,\n",
       "          0.03609927, -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "          1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        , -1.        ,  1.        ,  1.        ,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "          1.        , -1.        ,  1.        , -1.        ,\n",
       "          1.        ,  1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        , -0.98308754,  0.09617945,\n",
       "         -1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        ,  0.89532197, -1.        , -1.        ,\n",
       "          1.        , -1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "          1.        , -1.        , -1.        , -0.19820556,\n",
       "         -1.        , -1.        ,  1.        ,  1.        ,\n",
       "          1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  0.9976674 ,  1.        , -1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "          1.        ,  1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  1.        ,  1.        , -1.        ,\n",
       "         -1.        ,  0.18183126, -1.        , -1.        ,\n",
       "          1.        , -0.15561043,  1.        , -1.        ,\n",
       "         -1.        , -0.7212484 ,  1.        , -1.        ,\n",
       "          1.        , -1.        ,  1.        ,  1.        ,\n",
       "         -1.        ,  1.        , -1.        ,  1.        ,\n",
       "          1.        ,  1.        ,  1.        , -1.        ,\n",
       "         -1.        , -1.        ,  1.        ,  1.        ,\n",
       "          1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        ,  1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        ,  1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ,\n",
       "         -1.        , -1.        , -1.        , -1.        ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ce2b8985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.880e+02, 2.880e+02, 2.530e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.000e+00, 5.000e+00, 6.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [8.520e+02, 7.770e+02, 3.301e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [9.100e+01, 1.087e+03, 2.660e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [2.190e+02, 1.568e+03, 1.640e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.530e+02, 2.400e+01, 4.900e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "64c3de42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['어',\n",
       " '그리고',\n",
       " '그때',\n",
       " '는',\n",
       " '그런',\n",
       " '직접',\n",
       " '적',\n",
       " '인',\n",
       " '지원',\n",
       " '도',\n",
       " '물론',\n",
       " '필요하지만',\n",
       " '그래도',\n",
       " '다',\n",
       " '같이',\n",
       " '모',\n",
       " '여서',\n",
       " '예',\n",
       " '를',\n",
       " '들어',\n",
       " '부업',\n",
       " '을',\n",
       " '한다거나',\n",
       " '그런',\n",
       " '단순한',\n",
       " '작업',\n",
       " '을',\n",
       " '하면서',\n",
       " '그런',\n",
       " '소득',\n",
       " '을',\n",
       " '얻는',\n",
       " '방식',\n",
       " '그러면은',\n",
       " '어르신',\n",
       " '분',\n",
       " '들',\n",
       " '도',\n",
       " '적적하지',\n",
       " '않으실',\n",
       " '거',\n",
       " '같고',\n",
       " '서로',\n",
       " '소통',\n",
       " '하면서',\n",
       " '치매',\n",
       " '예방',\n",
       " '이나',\n",
       " '그런',\n",
       " '우울증',\n",
       " '예방',\n",
       " '그런',\n",
       " '것',\n",
       " '도',\n",
       " '좋을',\n",
       " '거',\n",
       " '같아요',\n",
       " '.',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[reverse[i] for i in y[0].cpu().detach().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32c594aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:354\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m(f\u001b[38;5;241m.\u001b[39mtell())\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Untitled Folder/Text2Text.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:276\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _open_buffer_writer(name_or_buffer)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_buffer_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in mode but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:261\u001b[0m, in \u001b[0;36m_open_buffer_reader.__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, buffer):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(buffer)\n\u001b[0;32m--> 261\u001b[0m     \u001b[43m_check_seekable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:357\u001b[0m, in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (io\u001b[38;5;241m.\u001b[39mUnsupportedOperation, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 357\u001b[0m     \u001b[43mraise_err_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseek\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/virtual/lib/python3.8/site-packages/torch/serialization.py:350\u001b[0m, in \u001b[0;36m_check_seekable.<locals>.raise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m    347\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. You can only torch.load from a file that is seekable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please pre-load the data into a buffer like io.BytesIO and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m                         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m try to load from it instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 350\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(msg)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."
     ]
    }
   ],
   "source": [
    "model = torch.load(model.state_dict(), \"./Untitled Folder/Text2Text.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"밥은 먹었어?\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://ratsgo.github.io/nlpbook/docs/generation/inference1/\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty = 1.5,\n",
    "        no_repeat_ngram_size=3,\n",
    "        top_k=50, \n",
    "        temperature=0.9, ## 1보다 작게 하면 정확한 문장을, 1보다 크게 하면 다양한 문장 생성\n",
    "        top_p = 0.92,\n",
    "      )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788dd48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://ratsgo.github.io/nlpbook/docs/generation/inference2/#%EC%BD%94%EB%93%9C6-%EC%9D%B8%ED%8D%BC%EB%9F%B0%EC%8A%A4\n",
    "\n",
    "def inference_fn(\n",
    "        prompt,\n",
    "        min_length=10,\n",
    "        max_length=20,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.0,\n",
    "        no_repeat_ngram_size=0,\n",
    "        temperature=1.0,\n",
    "):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True,\n",
    "                top_p=float(top_p),\n",
    "                top_k=int(top_k),\n",
    "                min_length=int(min_length),\n",
    "                max_length=int(max_length),\n",
    "                repetition_penalty=float(repetition_penalty),\n",
    "                no_repeat_ngram_size=int(no_repeat_ngram_size),\n",
    "                temperature=float(temperature),\n",
    "           )\n",
    "        generated_sentence = tokenizer.decode([el.item() for el in generated_ids[0]])\n",
    "    except:\n",
    "        generated_sentence = \"\"\"처리 중 오류가 발생했습니다. <br>\n",
    "            변수의 입력 범위를 확인하세요. <br><br> \n",
    "            min_length: 1 이상의 정수 <br>\n",
    "            max_length: 1 이상의 정수 <br>\n",
    "            top-p: 0 이상 1 이하의 실수 <br>\n",
    "            top-k: 1 이상의 정수 <br>\n",
    "            repetition_penalty: 1 이상의 실수 <br>\n",
    "            no_repeat_ngram_size: 1 이상의 정수 <br>\n",
    "            temperature: 0 이상의 실수\n",
    "            \"\"\"\n",
    "    return {\n",
    "        'result': generated_sentence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cfd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.qa import get_web_service_app\n",
    "app = get_web_service_app(inference_fn)\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
