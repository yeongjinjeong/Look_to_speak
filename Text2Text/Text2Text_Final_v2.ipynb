{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6790c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce00986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import font_manager\n",
    "\n",
    "# # 설치할 폰트 파일의 경로와 이름을 지정합니다.\n",
    "# font_path = './NanumGothic.ttf'\n",
    "# font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "# # 폰트를 설치한 폴더에 있는 폰트 파일의 이름을 출력합니다.\n",
    "# print(font_name)\n",
    "\n",
    "# # Unicode warning 제거 (폰트 관련 경고메시지)\n",
    "# plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "# # 기본 폰트를 설치한 폰트로 설정합니다.\n",
    "# plt.rcParams['font.family'] = font_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e660dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 사전 생성\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1b0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVocab():\n",
    "    def __init__(self):\n",
    "        SOS_TOKEN = 0\n",
    "        EOS_TOKEN = 1\n",
    "        UNKNOWN_TOKEN = 2\n",
    "        \n",
    "        self.unknown_token = UNKNOWN_TOKEN\n",
    "        \n",
    "        # 각 토큰 별 word count\n",
    "        self.word2count = {}\n",
    "        \n",
    "        # word -> idx\n",
    "        self.word2index = {\n",
    "            '<SOS>': SOS_TOKEN, \n",
    "            '<EOS>': EOS_TOKEN,\n",
    "            '<UKN>': UNKNOWN_TOKEN,\n",
    "        }\n",
    "\n",
    "        # idx -> word\n",
    "        self.index2word = {\n",
    "            SOS_TOKEN: '<SOS>', \n",
    "            EOS_TOKEN: '<EOS>', \n",
    "            UNKNOWN_TOKEN: '<UKN>',\n",
    "        }\n",
    "        \n",
    "        # total word counts\n",
    "        self.n_words = 3  # SOS, EOS, UNKNOWN 포함\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def word_to_index(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        else:\n",
    "            return self.unknown_token\n",
    "    \n",
    "    def index_to_word(self, idx):\n",
    "        return self.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d41102e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvocab = WordVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab43a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 프로세스를 클레스화 - 데이터 로드, 전처리, 사전 생성, 시퀀스 변환\n",
    "class QADataset():\n",
    "    def __init__(self, csv_path, min_length=1, max_length=647):\n",
    "        data_dir = 'data'\n",
    "        \n",
    "        # TOKEN 정의\n",
    "        self.SOS_TOKEN = 0 # SOS 토큰\n",
    "        self.EOS_TOKEN = 1 # EOS 토큰\n",
    "        \n",
    "        self.tagger = Okt()   # 형태소 분석기\n",
    "        self.max_length = max_length # 한 문장의 최대 길이 지정\n",
    "        \n",
    "        # CSV 데이터 로드\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # 한글 정규화\n",
    "        korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "        self.normalizer = re.compile(korean_pattern)\n",
    "        \n",
    "        # src: 질의, tgt: 답변\n",
    "        src_clean = []\n",
    "        tgt_clean = []\n",
    "        \n",
    "        # 단어 사전 생성\n",
    "        wordvocab = WordVocab()\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            src = row['value']\n",
    "            tgt = row['label']\n",
    "            \n",
    "            # 한글 전처리\n",
    "            src = self.clean_text(src)\n",
    "            tgt = self.clean_text(tgt)\n",
    "            \n",
    "            if len(src.split()) > min_length and len(tgt.split()) > min_length:\n",
    "                # 최소 길이를 넘어가는 문장의 단어만 추가\n",
    "                wordvocab.add_sentence(src)\n",
    "                wordvocab.add_sentence(tgt)\n",
    "                src_clean.append(src)\n",
    "                tgt_clean.append(tgt)            \n",
    "        \n",
    "        self.srcs = src_clean\n",
    "        self.tgts = tgt_clean\n",
    "        self.wordvocab = wordvocab\n",
    "        \n",
    "\n",
    "    \n",
    "    def normalize(self, sentence):\n",
    "        # 정규표현식에 따른 한글 정규화\n",
    "        return self.normalizer.sub(\"\", sentence)\n",
    "\n",
    "    def clean_text(self, sentence):\n",
    "        # 한글 정규화\n",
    "        sentence = self.normalize(str(sentence))\n",
    "        # 형태소 처리\n",
    "        sentence = self.tagger.morphs(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        return sentence\n",
    "    \n",
    "    def texts_to_sequences(self, sentence):\n",
    "        # 문장 -> 시퀀스로 변환\n",
    "        sequences = [self.wordvocab.word_to_index(w) for w in sentence.split()]\n",
    "        # 문장 최대 길이 -1 까지 슬라이싱\n",
    "        sequences = sequences[:self.max_length-1]\n",
    "        # 맨 마지막에 EOS TOKEN 추가\n",
    "        sequences.append(self.EOS_TOKEN)\n",
    "        return sequences\n",
    "    \n",
    "    def sequences_to_texts(self, sequences):\n",
    "        # 시퀀스 -> 문장으로 변환\n",
    "        sentences = [self.wordvocab.index_to_word(s.item()) for s in sequences]\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.srcs[idx]\n",
    "        inputs_sequences = self.texts_to_sequences(inputs)\n",
    "        \n",
    "        outputs = self.tgts[idx]\n",
    "        outputs_sequences = self.texts_to_sequences(outputs)\n",
    "        \n",
    "        return torch.tensor(inputs_sequences).view(-1, 1), torch.tensor(outputs_sequences).view(-1, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d13dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 358\n",
    "\n",
    "dataset = QADataset('./reduce_df.csv', min_length=1, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ae704",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "982daedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        \"\"\"\n",
    "        sin, cos encoding 구현\n",
    "        \n",
    "        parameter\n",
    "        - d_model : model의 차원\n",
    "        - max_len : 최대 seaquence 길이\n",
    "        - device : cuda or cpu\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__() # nn.Module 초기화\n",
    "        \n",
    "        # input matrix(자연어 처리에선 임베딩 벡터)와 같은 size의 tensor 생성\n",
    "        # 즉, (max_len, d_model) size\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False # 인코딩의 그래디언트는 필요 없다. \n",
    "        \n",
    "        # 위치 indexing용 벡터\n",
    "        # pos는 max_len의 index를 의미한다.\n",
    "        pos = torch.arange(0, max_len, device =device)\n",
    "        # 1D : (max_len, ) size -> 2D : (max_len, 1) size -> word의 위치를 반영하기 위해\n",
    "        \n",
    "        pos = pos.float().unsqueeze(dim=1) # int64 -> float32 (없어도 되긴 함)\n",
    "        \n",
    "        # i는 d_model의 index를 의미한다. _2i : (d_model, ) size\n",
    "        # 즉, embedding size가 512일 때, i = [0,512]\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        \n",
    "        # (max_len, 1) / (d_model/2 ) -> (max_len, d_model/2)\n",
    "        self.encoding[:, ::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # self.encoding\n",
    "        # [max_len = 512, d_model = 512]\n",
    "\n",
    "        # batch_size = 128, seq_len = 30\n",
    "        batch_size, seq_len = x.size() \n",
    "        \n",
    "        # [seq_len = 30, d_model = 512]\n",
    "        # [128, 30, 512]의 size를 가지는 token embedding에 더해질 것이다. \n",
    "        # \n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "025f399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding_de(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        \"\"\"\n",
    "        sin, cos encoding 구현\n",
    "        \n",
    "        parameter\n",
    "        - d_model : model의 차원\n",
    "        - max_len : 최대 seaquence 길이\n",
    "        - device : cuda or cpu\n",
    "        \"\"\"\n",
    "        \n",
    "        super(PositionalEncoding_de, self).__init__() # nn.Module 초기화\n",
    "        \n",
    "        # input matrix(자연어 처리에선 임베딩 벡터)와 같은 size의 tensor 생성\n",
    "        # 즉, (max_len, d_model) size\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False # 인코딩의 그래디언트는 필요 없다. \n",
    "        \n",
    "        # 위치 indexing용 벡터\n",
    "        # pos는 max_len의 index를 의미한다.\n",
    "        pos = torch.arange(0, max_len, device =device)\n",
    "        # 1D : (max_len, ) size -> 2D : (max_len, 1) size -> word의 위치를 반영하기 위해\n",
    "        \n",
    "        pos = pos.float().unsqueeze(dim=1) # int64 -> float32 (없어도 되긴 함)\n",
    "        \n",
    "        # i는 d_model의 index를 의미한다. _2i : (d_model, ) size\n",
    "        # 즉, embedding size가 512일 때, i = [0,512]\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        \n",
    "        # (max_len, 1) / (d_model/2 ) -> (max_len, d_model/2)\n",
    "        self.encoding[:, ::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # self.encoding\n",
    "        # [max_len = 512, d_model = 512]\n",
    "\n",
    "        # batch_size = 128, seq_len = 30\n",
    "        batch_size, seq_len , d_model= x.size() \n",
    "        \n",
    "        # [seq_len = 30, d_model = 512]\n",
    "        # [128, 30, 512]의 size를 가지는 token embedding에 더해질 것이다. \n",
    "        # \n",
    "        return x + self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2139e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 단어 사전의 개수 지정\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, MAX_LENGTH, device)\n",
    "        self.num_vocabs = num_vocabs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        # 임베딩 레이어 정의 (number of vocabs, embedding dimension)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "       \n",
    "        # GRU (embedding dimension)\n",
    "        self.gru = nn.GRU(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          bidirectional=True, \n",
    "                          batch_first=True,\n",
    "                          dropout=0.2,\n",
    "                         )\n",
    "    def init_weights(self):\n",
    "        torch.nn.init.xavier_uniform_(self.transformer_encoder.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(self.gru.weight.data)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "\n",
    "        # (sequence_length, 1)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, device):\n",
    "        return torch.zeros(6, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c0fe9",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f67d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_vocabs, hidden_size, embedding_dim, dropout_p=0.2, max_length=MAX_LENGTH):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding_de(embedding_dim, MAX_LENGTH, device)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(embedding_dim, nhead=8)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "        self.embedding = nn.Embedding(num_vocabs, embedding_dim)\n",
    "        self.attn = nn.Linear(hidden_size + embedding_dim, max_length)\n",
    "#         self.attn_combine = nn.Linear(hidden_size*3, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                          hidden_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, num_vocabs)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for i in range(self.gru.num_layers):\n",
    "            torch.nn.init.xavier_uniform_(getattr(self.gru, f'weight_hh_l{i}'))\n",
    "            torch.nn.init.xavier_uniform_(getattr(self.gru, f'weight_ih_l{i}'))\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(x).view(1, 1, -1)\n",
    "\n",
    "        x = embedded\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "\n",
    "        # Using last layer of GRU for attention mechanism\n",
    "        hidden_last_layer = hidden[-1].unsqueeze(0)\n",
    "        \n",
    "#         print(embedded.shape, hidden_last_layer.shape)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden_last_layer), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        encoder_output = attn_applied\n",
    "#         output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "   \n",
    "        output = self.transformer_decoder(x, encoder_output)\n",
    "        output = F.relu(output)\n",
    "        hidden = hidden[0].unsqueeze(0)\n",
    "        output, hidden = self.gru(output[0], hidden)\n",
    "        output = F.log_softmax(self.out(output), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(5, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db84ce",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = dataset.SOS_TOKEN\n",
    "EOS_TOKEN = dataset.EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a183fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련시 training loss 를 출력하기 위한 util 함수\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # 주기적인 간격에 이 locator가 tick을 설정\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.title('Losses over training')\n",
    "    plt.show()\n",
    "    \n",
    "# 훈련시 시간 출력을 위한 util 함수\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f'{int(m)}m {int(s)}s'\n",
    "\n",
    "# 훈련시 시간 출력을 위한 util 함수\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return f'{as_minutes(s)} (remaining: {as_minutes(rs)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25171e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "          decoder_optimizer, criterion, device, max_length=MAX_LENGTH, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # Encoder의 hidden_state 초기화\n",
    "    encoder_hidden = encoder.init_hidden(device=device)\n",
    "    \n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # input_length: 입력 문장의 길이\n",
    "    # target_length: 출력 문장의 길이\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Encoder의 출력 결과를 담을 tensor\n",
    "    # (문장의 max_length, encoder의 hidden_size)\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "#     print(encoder_outputs.shape) ## (647,512)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        # Encoder의 출력을 encoder_outputs[ei] 에 저장\n",
    "        # encoder_output[0, 0]: (hidden_size,)\n",
    "#         print(encoder_output.shape) ## (1,1,2*512)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # Decoder의 첫 토큰은 SOS_TOKEN\n",
    " \n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]], device=device)\n",
    "  \n",
    "    # Encoder의 마지막 hidden state를 Decoder의 초기 hidden state로 지정\n",
    "    decoder_hidden = encoder_hidden\n",
    "  \n",
    "    # teacher forcing 적용 여부 확률로 결정\n",
    "    # teacher forcing 이란: 정답치를 다음 RNN Cell의 입력으로 넣어주는 경우. 수렴속도가 빠를 수 있으나, 불안정할 수 있음\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "   \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # loss 계산\n",
    "        loss += criterion(decoder_output.view(1, -1), target_tensor[di])\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            # teacher forcing 적용: 정답 값 입력\n",
    "            decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            # 확률, 인덱스\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # 다음 입력으로 주입할 디코더 최종 토큰 결정\n",
    "            decoder_input = topi.squeeze().detach()  # 입력으로 사용할 부분을 히스토리에서 분리\n",
    "\n",
    "        # EOS_TOKEN 이면 종료\n",
    "        if decoder_input.item() == EOS_TOKEN:\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef9c1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iterations(encoder, decoder, n_iters, dataset, device, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # print_every 마다 초기화\n",
    "    plot_loss_total = 0  # plot_every 마다 초기화\n",
    "\n",
    "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate, betas=(0.9,0.98))\n",
    "    decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate, betas=(0.9,0.98))\n",
    "    \n",
    "    encoder_scheduler = optim.lr_scheduler.StepLR(optimizer=encoder_optimizer,\n",
    "                                            step_size=10.0,\n",
    "                                            gamma=0.95,\n",
    "                                            verbose=False)\n",
    "    decoder_scheduler = optim.lr_scheduler.StepLR(optimizer=decoder_optimizer,\n",
    "                                                 step_size=10.0,\n",
    "                                                 gamma=0.95,\n",
    "                                                 verbose=False)\n",
    "    \n",
    "    # 랜덤 샘플링된 데이터셋 생성\n",
    "    training_pairs = [dataset[random.randint(0, len(dataset)-1)] for i in range(n_iters)]\n",
    "    \n",
    "    # Loss Function 정의\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # n_iters 만큼 training 시작\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        # 문장 pair\n",
    "        training_pair = training_pairs[iter - 1]        \n",
    "        # 입력 문장\n",
    "        input_tensor = training_pair[0]\n",
    "        # 출력 문장\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        input_tensor = input_tensor.to(device)\n",
    "        target_tensor = target_tensor.to(device)\n",
    "\n",
    "        # 훈련\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
    "                     decoder_optimizer, criterion, device)\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        # print_every 마다 loss 출력, 모델 저장\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(f'{time_since(start, iter/n_iters)} iter: {iter} ({iter/n_iters*100:.1f}%), loss: {print_loss_avg:.4f}')\n",
    "            torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                }, \"./model/Text2Text-se2se.pt\")\n",
    "\n",
    "            \n",
    "        # plot_every 마다 loss 시각화\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        \n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe153542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter 정의\n",
    "NUM_VOCABS = dataset.wordvocab.n_words\n",
    "HIDDEN_SIZE = 256\n",
    "EMBEDDING_DIM = 256\n",
    "DROPOUT_P = 0.3\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# Encoder 정의\n",
    "encoder = Encoder(NUM_VOCABS, \n",
    "                  hidden_size=HIDDEN_SIZE, \n",
    "                  embedding_dim=EMBEDDING_DIM, \n",
    "                  num_layers=NUM_LAYERS)\n",
    "\n",
    "# Attention 이 적용된 Decoder 정의\n",
    "decoder = Decoder(num_vocabs=NUM_VOCABS, \n",
    "                           hidden_size=HIDDEN_SIZE, \n",
    "                           embedding_dim=EMBEDDING_DIM, \n",
    "                           dropout_p=DROPOUT_P, \n",
    "                           max_length=MAX_LENGTH)\n",
    "\n",
    "# encoder, decoder 생성 및 device 지정\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterations(encoder, decoder, 200, dataset, device, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef3f20",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42029d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, dataset, device, max_length=MAX_LENGTH):\n",
    "    # Eval 모드 설정\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        # Encoder의 hidden state 초기화\n",
    "        encoder_hidden = encoder.init_hidden(device=device)\n",
    "  \n",
    "\n",
    "        # encoder_outputs는 Encoder를 통과한 문장의 출력\n",
    "        # (max_length, hidden_size)\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    " \n",
    "        \n",
    "        # Encoder 에 입력 문자 주입 후 encoder_outputs 생성\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        # Decoder의 첫 번째 입력으로 SOS_TOKEN 입력(SOS_TOKEN=0)\n",
    "        decoder_input = torch.tensor([[0]], device=device)\n",
    "  \n",
    "        # Decoder의 첫 번째 hidden state는 Encoder의 마지막 hidden state 사용\n",
    "        decoder_hidden = encoder_hidden\n",
    "    \n",
    "\n",
    "        decoded_words = []\n",
    "      \n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "      \n",
    "        for di in range(max_length):\n",
    "            # 1개의 Decoder 입력 토큰을 통과\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Attention 시각화를 위한 tensor 저장\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            # 출력 토큰 예측\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            \n",
    "            # EOS_TOKEN이면 종료\n",
    "            if topi.item() == dataset.EOS_TOKEN:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                # 출력 문장에 토큰 시퀀스(index)를 단어(word)로 변환한 후 저장\n",
    "                decoded_words.append(dataset.wordvocab.index_to_word(topi.item()))\n",
    "\n",
    "            # decoder_input은 다음 토큰 예측시 입력 값\n",
    "            # decoder_input: (hidden_size,)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ac74948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq(input_seq, dataset,max_length = MAX_LENGTH):\n",
    "    \n",
    "    okt = Okt()\n",
    "    \n",
    "    SOS_TOKEN = 0\n",
    "    EOS_TOKEN = 1\n",
    "    \n",
    "    korean_pattern = r'[^ ?,.!A-Za-z0-9가-힣+]'\n",
    "    normalizer = re.compile(korean_pattern)\n",
    "    \n",
    "    src = input_seq\n",
    "    src = normalizer.sub(\"\", str(src))\n",
    "    src = okt.morphs(src)\n",
    "    \n",
    "    sequences = [dataset.wordvocab.word_to_index(w) for w in src]\n",
    "    # 문장 최대 길이 -1 까지 슬라이싱\n",
    "    sequences = sequences[:max_length-1]\n",
    "        # 맨 마지막에 EOS TOKEN 추가\n",
    "    sequences.append(EOS_TOKEN)\n",
    "    \n",
    "    return torch.tensor(sequences).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8712b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generate(input_seq,encoder, decoder, dataset, device):\n",
    "    src = input_seq\n",
    "    a = eval_seq(src,dataset)\n",
    "    output_words, attentions = evaluate(encoder, decoder, a.to(device), dataset, device)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "        \n",
    "        # 예측 문장 출력\n",
    "    print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49d2329f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나 는 는 는 가 가 <EOS>\n"
     ]
    }
   ],
   "source": [
    "text_generate(\"반려동물\",encoder, decoder, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc8fd0",
   "metadata": {},
   "source": [
    "attention 가중치 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c0fb8a34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1371.43x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Attention Weights를 활용한 시각화\n",
    "output_words, attentions = evaluate(encoder, decoder, test_seq.to(device), dataset, device)\n",
    "# plt.figure(figsize=(1000,1000))\n",
    "plt.matshow(attentions.numpy()[:,:20])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e175459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화를 위한 함수\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # colorbar로 그림 설정\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy()[:,:20], cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # 축 설정\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') , rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # 매 틱마다 라벨 보여주기\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_show_attention(encoder, decoder, input_sentence, dataset, device):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence.to(device), dataset, device)\n",
    "    input_sentence = dataset.sequences_to_texts(input_sentence)\n",
    "    output_words = ' '.join(output_words)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', output_words)\n",
    "    show_attention(input_sentence, output_words.split(), attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "348ff97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = eval_seq(\"반려동물\",dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "951258db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = 반려동물 <EOS>\n",
      "output = 나 는 는 는 가 가 <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAikAAAGFCAYAAAA4pWMWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm4klEQVR4nO3dfXRU9Z3H8c8ETEYJGXEpSYEoRYVIVVBC0rgHYo+xcbuyUh82oUiy6UrTU+wpjR5L6h7AhxJ8KMYGKhQfqNpWtOoRY8WtcQNHRSy4nAq6aGtTxodEsJqBECaQmf0jydiRBGdyb7z3N/f9yrnHzp17f/c7AZJvv78nXzQajQoAAMBl0pwOAAAAoD8kKQAAwJVIUgAAgCuRpAAAAFciSQEAAK5EkgIAAFyJJAUAALjScKcDAAAAiTl8+LC6urost5Oeni6/329DREOLJAUAAAMcPnxYX/nKV9Ta2mq5rZycHP31r391faJCkgIAgAG6urrU2tqqYDCorKysQbcTCoWUm5urrq4ukhQAAGCfkSNHauTIkYO+36TdcEhSAAAwSCQaVcRComHl3i8as3sAAIArUUkBAMAg0WjUUpcN3T0AAGBIRHu/rNxvCrp7AACAK1FJAQDAIJFoz2HlflOQpAAAYBAvjUmhuwcAALgSlRQAAAzipXVSSFIAADCIl7p7SFIAADCIl5IUxqQAAABXopICAIBBGJMCAABcie4eAAAAh1FJAQDAIF7au4ckBQAAg3hpWXy6ewAAgCtRSQEAwCQWB87KoIGzJCkAABjES1OQ6e4BAACuRCUFAACDeGmdFJIUAAAMQpICAABciTEpAAAADqOSAgCAQejuAQAAruSlZfHp7gEAAK5EJQUAAIN4ae8ekhQAAAwSlbVxJQblKHT3AAAAd6KSAgCAQZjdAwAAXMlLi7mRpAAAYBAvVVIYkwIAAFyJSgoAAAahuwcAALiTxe4eGZSk0N0DAABciUoKAAAG8dLePSQpAAAYxEvL4tPdAwAAXIlKCgAABvHSOikkKQAAGMRLSQrdPQAAwJWopAAAYBAWcwMAAK7kpe4ekhQAAAzipSSFMSkAAMCVqKQAAGAQxqQAAABX8tKy+HT3AAAAV6KSAgCAQby0dw9JCgAABmF2DwAAgMOopAAAYBAqKQAAwJWivVOQB3sMNklZvXq1JkyYIL/fr8LCQr366qsJ3ffII4/I5/Npzpw5ST+TJMVluru79ac//UlHjx51OhQAACRJGzZsUE1NjZYuXarXXntNU6dOVWlpqT788MPj3tfS0qLrr79eM2fOHNRzSVJc5umnn9Z5552nDRs2OB0KAMCF+rp7rBzJWrlypRYsWKCqqipNmTJFa9as0UknnaT7779/wHu6u7s1b9483XTTTZo4ceKgPitJisv86le/0pe+9CWtX7/e6VAAAC4UlcVEpbedUCgUd4TD4X6f19XVpR07dqikpCR2Li0tTSUlJdq6deuAcd58880aM2aM/vM//3PQn5UkxUX279+vZ599VuvXr9fmzZv17rvvOh0SAMBlrIxH+ccl9XNzcxUIBGJHXV1dv8/bv3+/uru7lZ2dHXc+Oztbra2t/d7z4osv6r777tO6dessfVZm97jIb3/7W5199tm65JJLNHPmTD300EOqra11OiwAQAoKBoPKysqKvc7IyLCl3QMHDmj+/Plat26dRo8ebaktkhQXWb9+vSorKyVJV199tW6//XaSFABAHLv27snKyopLUgYyevRoDRs2TG1tbXHn29ralJOTc8z1f/nLX9TS0qLZs2fHzkUiEUnS8OHDtWfPHp1++ukJxUp3j0vs2rVLu3bt0re//W1J0lVXXaW9e/dq27ZtDkcGAHCTvmXxrRzJSE9P1/Tp09XU1PRpDJGImpqaVFRUdMz1eXl5ev3117Vz587Y8W//9m/6+te/rp07dyo3NzfhZ1NJcYlf/epX+sY3vhErjWVmZmrOnDlav369CgsLHY4OAOBlNTU1qqysVH5+vgoKClRfX6+Ojg5VVVVJkioqKjRu3DjV1dXJ7/fr7LPPjrv/5JNPlqRjzn8ekhQX6O7u1sMPP6yf//znceevvvpqzZs3T3fffbfS09Mdig4A4CZOrDhbVlamffv2acmSJWptbdW0adO0adOm2GDavXv3Ki3N/s4ZX9Sk9XFT1AcffKB169Zp8eLFcclIJBLR8uXLVVFRoVNPPdXBCAEATguFQgoEAmp89VWNyMwcdDsdBw/q0oICtbe3JzQmxUkkKQAAGMCLSQrdPS71t7/9TR0dHcrLyxuSEhoAwEz/uNbJYO83Bb/9HHb//fdr5cqVcee++93vauLEiTrnnHN09tlnKxgMOhQdAMBtnFgW3ykkKQ775S9/qVGjRsVeb9q0SQ888IAefPBB/fGPf9TJJ5+sm266ycEIAQBwBt09Dnv77beVn58fe/3UU0/psssu07x58yRJy5cvj03xAgDAidk9TqGS4rDOzs64gUsvv/yyZs2aFXs9ceLEAfdGAAB4j11795iAJMVhp512mnbs2CGpZxOn3bt365//+Z9j77e2tioQCDgVHgDAZaI2fJmC7p4kXHHFFfrggw8Svn7KlCm69957j3tNZWWlFi5cqN27d+uFF15QXl6epk+fHnv/5ZdfTnqFPgAAUgFJShLeeecd/e///m/C1xcUFHzuNTfccIMOHTqkJ554Qjk5OXrsscfi3n/ppZc0d+7cpGMFAKSmaLTnsHK/KUhSkuDz+WxvMy0tTTfffLNuvvnmft//bNICAPC2qMVxJSYNnCVJcYnOzk794Q9/0FtvvSVJmjRpki6++GKdeOKJDkcGAIAzSFJcYOPGjbrmmmu0f//+uPOjR4/Wfffdp9mzZzsUGQDAbZiCjC/Myy+/rCuvvFKzZs3SSy+9pL///e/6+9//rhdffFEzZ87UlVdeqVdeecXpMAEALuGlKchsMJiEyZMnx00PPp5oNKpdu3bpj3/843Gv++Y3v6nc3FytXbu23/erq6sVDAb1+9//Pul4AQCpo2+DwUe2bNFJFjYYPHTwoMpnzWKDwVTz7LPP6siRIwlfn8h4kldeeUW33XbbgO8vXLhQxcXFCT8TAJDavNTdQ5KShG3btunAgQMJXz9mzBideuqpx73msyvOflYgENDhw4cTfiYAILV5KUlhTEoSfvrTn8rv9ysjIyOhY/ny5Z/b5plnnqkXXnhhwPebmpp05pln2vkxAAAwApWUJJxwwgmqqKhI+PpVq1Z97jVVVVW6/vrrlZ2drW9+85tx7z3zzDO64YYb9JOf/CTpWAEAqcnq4FeTBs6SpCQh2cXcErn+hz/8oV5++WVdeumlmjx5ss466yxFo1G9+eabevvttzVnzhwtWrRokBEDAFKN1f13TNq7h+4eh6Wlpemxxx7Tb3/7W02ePFn/93//pz179igvL0+//vWv9fjjjystjT8mAECPvmXxrRymoJLiEmVlZSorK3M6DAAAXIMkJQlHjhzRli1bEro20dHXjz76qObMmaP09HRJ0rvvvquxY8fGqieHDh3SqlWrdMMNNww+cABAyvDSmBQWc0vC7bffro8//jjh68ePH6+FCxce95phw4bpgw8+0JgxYyRJWVlZ2rlzpyZOnChJamtr09ixY9Xd3T34wAEAxutbzO2BpiadNGLEoNs51NGhqosuYjG3VPOjH/0oqfnliYwl+Wx75IwAAPQgSUnCV7/6VY0fPz6ha6PRqA4dOqRt27YNcVQAAC/xUncPSUoSRowYcdyF1z5rxowZQxgNAMCLvLTiLElKEoZinRRJeu655xQIBCRJkUhETU1N2rVrlyTpk08+SeqZAACkCpIUF6isrIx7XV1dHfc62eQIAJC6qKTgCxOJRJwOAQBgEqsrshmUpLCUqQscOnRIr7/+er/v7d69WwcPHvyCIwIAwHlUUpKQnp6uCy64IOHrR48endB1XV1dKiwsVHNzswoKCmLn33jjDZ133nnau3evMjMzk44XAJB6opGoohEL3T0W7v2ikaQkoaCgQPv27Uv4+jPOOCOh604++WRdeumlevDBB+OSlIceekgXXXSRcnJyko4VAJCirO6/Y06OQpKSjC1btmjjxo0JDzq66qqrdMsttyR0bWVlpf7jP/5D9fX1Gj58uKLRqH7961/rzjvvtBIyACDFMHAW/fL5fDr11FMTvj6ZvwiXXHKJhg8frmeeeUaXXXaZmpubdfDgQc2ZM2cQkQIAYD4GziZhqNZJkXr28Jk3b54efPBBST1dPWVlZbGNBwEAkD6tpFg5TEElxUUqKytVUFCg9957T48//riee+45p0MCALiMl7p7qKS4yDnnnKMpU6Zo3rx5+vKXv6yvfe1rTocEAIBjqKQkobOzUzfffHNC1w42U62oqNCPfvQj3XrrrYO6HwCQ2piCjH6tXbtWnZ2dCV9fWlqa9DPmz5+vTz75RN/5zneSvhcAkPq81N1DkpKEWbNmDfkzTjnlFC1dunTInwMAgNuRpAAAYBAqKQAAwJ3YYBCJCofDWrZsmcLhMO053J6bY6M9d7Xn5thozz1tDUV7SI4valLdx4VCoZACgYDa29uVlZVFew625+bYaM9d7bk5NtpL7T9bO2JpePwpnThixKDb6ezo0A+uuMwVn+nz0N0DAIBBolGLU5ANqk2QpAAAYBAGzkKRSETvv/++Ro4cedw9eEKhUNx/raI9d7RFe6ndnptjoz33tJVMe9FoVAcOHNDYsWOVlsZwT7swJmUA7777rnJzc50OAwBgkGAwqPHjxw9J231jUuoffUInnmRhTMqhDi3698sZk2KykSNHOh0CAMAwX8TvDi9191CTGsDxungAAOgPvzvsRSUFAACDeKmSQpICAIBJIpKs7GQcsS2SIUd3DwAAcKWUraRs3rxZ1dXV8vv9cecjkYiKi4vV0NDgUGQAAAwe3T0poLOzU+Xl5Vq2bFnc+ZaWFi1evNiZoAAAsMhD+wvS3QMAANwpZSspyQqHw3G7XNq1WiEAAHbyUncPlZRedXV1CgQCsYPVZgEAbtSXpFg5TEGS0qu2tlbt7e2xIxgMOh0SAADHiEailg9T0N3TKyMjQxkZGU6HAQAAepGkAABgEqtdNgZ195CkAABgEAbOAgAAOIxKCgAABvFSJYUkBQAAk3hoydmUTVICgYAaGxvV2Nh4zHulpaUORAQAAJKRsklKUVGRtm/f7nQYAADYKhrpOazcPxirV6/WHXfcodbWVk2dOlUNDQ0qKCjo99onnnhCy5cv15///GcdOXJEZ555pq677jrNnz8/qWembJLiRsOG2fvt9vnsHfd89OgRW9vzEp/PZ2t7dvcZp6XZ93clErHw07Efbv/e2R2f3ewfX2Bne+7+3tnri+tCicrimJRBxLphwwbV1NRozZo1KiwsVH19vUpLS7Vnzx6NGTPmmOtPOeUU3XjjjcrLy1N6eroaGxtVVVWlMWPGJNWbweweAABwXCtXrtSCBQtUVVWlKVOmaM2aNTrppJN0//3393v9hRdeqG9961s666yzdPrpp+uHP/yhzj33XL344otJPZckBQAAg9i1d08oFIo7/nGT3X/U1dWlHTt2qKSkJHYuLS1NJSUl2rp1a0LxNjU1ac+ePZo1a1ZSn5UkBQAAg9iVpOTm5sZtrFtXV9fv8/bv36/u7m5lZ2fHnc/OzlZra+uAcba3tyszM1Pp6en613/9VzU0NOjiiy9O6rMyJgUAAIPYtU5KMBhUVlZW7Lzd+9eNHDlSO3fu1MGDB9XU1KSamhpNnDhRF154YcJtpGySsnnzZlVXV8vv98edj0QiKi4uVkNDg0ORAQDgvKysrLgkZSCjR4/WsGHD1NbWFne+ra1NOTk5A96XlpamM844Q5I0bdo0vfnmm6qrq0sqSUnZ7p7Ozk6Vl5dr586dccfGjRu1b98+p8MDAGBQopGo5SMZ6enpmj59upqammLnIpGImpqaVFRUlHA7kUhkwHEvA0nZSgoAACnJgRVna2pqVFlZqfz8fBUUFKi+vl4dHR2qqqqSJFVUVGjcuHGxcS11dXXKz8/X6aefrnA4rN///vd66KGHdM899yT1XJIUAABwXGVlZdq3b5+WLFmi1tZWTZs2TZs2bYoNpt27d2/cekwdHR36/ve/r3fffVcnnnii8vLy9PDDD6usrCyp55Kk9AqHw3FlqFAo5GA0AAD0z6kNBq+99lpde+21/b7X3Nwc9/rWW2/VrbfeOqjn/KOUHZOSrLq6uripWLm5uU6HBADAMfp6e6wcpiBJ6VVbW6v29vbYEQwGnQ4JAABPo7unV0ZGhu1zxAEAsJtT3T1OIEkBAMAgg5lG/Nn7TUF3DwAAcCUqKQAAGITuHgAA4Eo9M3SsJCk2BjPEUjZJCQQCamxsVGNj4zHvlZaWOhARAADWUUlJAUVFRdq+fbvTYQAAgEFK2SQFAIBURCUFMWlpw+Tz+Wxpa/KkAlva6bP7jZdsbS8tbZhtbaWf4O41Z3w2flZJikSO2tre0aNHbG3P7x9hW1uRSMS2tiSpu9vez3rkSJet7dn177+P3d8/ye5fOPZ+XnuZ88t1SEWiPYeV+w3BFGQAAOBKVFIAADBIVNZm6JhTRyFJAQDALBbHpJg0B5nuHgAA4EopW0nZvHmzqqur5ff7485HIhEVFxeroaHBocgAABg8ZvekgM7OTpWXl2vZsmVx51taWrR48WJnggIAwCI2GAQAAHBYylZSkhUOhxUOh2OvQ6GQg9EAANA/L3X3UEnpVVdXp0AgEDtyc3OdDgkAgGP0JSlWDlOQpPSqra1Ve3t77AgGg06HBADAsXq2QbZ2GILunl4ZGRnKyHD3Uu4AAHgJSQoAAAbx0pgUkhQAAAwSjfQcVu43BWNSAACAK1FJAQDAIHT3AAAAVyJJSQGBQECNjY1qbGw85r3S0lIHIgIAAMlI2SSlqKhI27dvdzoMAABsRSUFAAC4EkkKYmz9w/T57GtL0ieHDtnanp2isvcfgU/2fu/s5vb47Px7HI1029aWNBQ/MO1tz6Qf6O7D9w7WkKQAAGCQaCSqaMRCJcXCvV80khQAAAxCdw8AAHApq5sEmpOksOIsAABwpZStpGzevFnV1dXy+/1x5yORiIqLi9XQ0OBQZAAADF7UYiHFoN6e1E1SOjs7VV5ermXLlsWdb2lp0eLFi50JCgAAi3qSFCtjUmwMZojR3QMAAFwpZSspyQqHwwqHw7HXoVDIwWgAAOifl6YgU0npVVdXp0AgEDtyc3OdDgkAgGP0TUG2cpiCJKVXbW2t2tvbY0cwGHQ6JAAAPI3unl4ZGRnKyMhwOgwAAI6LxdwAAIA7We2yMShJobsHAAC4EpUUAABM4qHV3EhSAAAwiJemIJOkAABgEA8VUlI3SQkEAmpsbFRjY+Mx75WWljoQEQAASEbKJilFRUXavn2702EAAGArpiDDCJ90dDgdwoDs/kfgS3P5RDSfz+kIjsvOP4+ozPkB5058/2CNl5IUl//kBwAAXkUlBQAAg3ipkkKSAgCAQbw0BZnuHgAA4EpUUgAAMAjdPSlg8+bNqq6ult/vjzsfiURUXFyshoYGhyIDAMAKi6u5GTTDLGWTlM7OTpWXl2vZsmVx51taWrR48WJnggIAAAlL2SQFAIBURHePB4XDYYXD4djrUCjkYDQAAPTPS3v3MLunV11dnQKBQOzIzc11OiQAAI7RNwXZymEKkpRetbW1am9vjx3BYNDpkAAA8DS6e3plZGQoIyPD6TAAADguxqQAAABX8lKSQncPAABwJSopAAAYxEuVFJIUAAAM0jMF2UqSYmMwQ4zuHgAA8LlWr16tCRMmyO/3q7CwUK+++uqA165bt04zZ87UqFGjNGrUKJWUlBz3+oGkbCUlEAiosbFRjY2Nx7xXWlrqQEQAAFhnda2Twdy7YcMG1dTUaM2aNSosLFR9fb1KS0u1Z88ejRkz5pjrm5ubNXfuXF1wwQXy+/267bbb9I1vfEO7d+/WuHHjEn6uL2pS59QXKBQKKRAIyOdLk8/ns6XNvLyv2dJOn2ean7S1vYnZOba1NXz4Cba1JUlpacNsbc/ns7eIGI1GbG3vyJHw51+UhPT0E21rKxI5altbPe3Z+707erTL1vbc/nfFfvb8vOvhvV8v7e3tysrKGpK2+34vlV99g9LTB79kRldXWI88fHtSsRYWFmrGjBlatWqVpJ5/t7m5ufrBD36Q0H543d3dGjVqlFatWqWKioqEY03ZSopd7Mzh5i78nm1tSdItS9fY2p6djh49Ymt7GRn2Jj12/yIbNcq+BE+SOjo+sbU9O/8ed3XZm0DZ9X8CPm3P7qTC7l+09n5eLyYCsMdnt38ZaL2wrq4u7dixQ7W1tbFzaWlpKikp0datWxN61qFDh3TkyBGdcsopScXImBQAAAzSt3ePlUOScnNz47aDqaur6/d5+/fvV3d3t7Kzs+POZ2dnq7W1NaGYf/zjH2vs2LEqKSlJ6rNSSQEAwCB2TUEOBoNx3T1Dter6ihUr9Mgjj6i5uVl+vz+pe0lSAAAwicUkpa+UkpWVldCYlNGjR2vYsGFqa2uLO9/W1qacnON3dd95551asWKFnn/+eZ177rlJh0p3DwAAGFB6erqmT5+upqam2LlIJKKmpiYVFRUNeN/tt9+uW265RZs2bVJ+fv6gnk0lBQAAgzgxBbmmpkaVlZXKz89XQUGB6uvr1dHRoaqqKklSRUWFxo0bFxvXctttt2nJkiX6zW9+owkTJsTGrmRmZiozMzPh56ZskrJ582ZVV1cf0/8ViURUXFyshoYGhyIDAGDwnFgWv6ysTPv27dOSJUvU2tqqadOmadOmTbHBtHv37lVa2qedM/fcc4+6urp05ZVXxrWzdOlSLVu2LOHnpmyS0tnZqfLy8mO+GS0tLQnN6QYAAJ+69tprde211/b7XnNzc9zrlpYWW56ZskkKAACpKCqLlRSD1tYhSekVDocVDn+6SNVnF7kBAMANvLQLMrN7etXV1cUtapObm+t0SAAAeBpJSq/a2lq1t7fHjmAw6HRIAAAcy64lZw1Ad0+vgfYsAADATaKRnsPK/aagkgIAAFyJSgoAAAbx0sBZkhQAAAxCkgIAAFzJS0kKY1IAAIArpWwlJRAIqLGxUY2Njce8V1pa6kBEAABY56VKSsomKUVFRdq+fbvTYQAAYCsndkF2SsomKW50pOuIve2F7W0Pg+fz+VzdXiRi0MIIANCLJAUAAJNYXTWW7h4AADAUor1fVu43BbN7AACAK1FJAQDAIMzuAQAArtSTpAx+MLxJScqQdvf4fL5+j0ceeSR2TXd3t+666y6dc8458vv9GjVqlP7lX/5FL730Ulxb3d3dWrFihfLy8nTiiSfqlFNOUWFhoe69996h/AgAAMAhtldSPv74Y51wwgnKzMyUJD3wwAO65JJL4q45+eSTJfVkc+Xl5Xr++ed1xx136KKLLlIoFNLq1at14YUX6rHHHtOcOXMkSTfddJPWrl2rVatWKT8/X6FQSNu3b9fHH38ca/f999/XmDFjNHw4BSIAQGqiuydJR48e1XPPPaf169fr6aef1rZt2zR16lRJPQlJTk5Ov/c9+uij+t3vfqeNGzdq9uzZsfO//OUv9dFHH+maa67RxRdfrBEjRmjjxo36/ve/r6uuuip2Xd8z+qxbt0733HOPrr76alVWVuqcc86x4+MBAOAaXkpSLHX3vP7667ruuus0fvx4VVRU6Etf+pL+53/+55jkYSC/+c1vNGnSpLgEpc91112njz76SH/4wx8kSTk5OXrhhRe0b9++Adv78Y9/rLvvvltvvvmmzj//fJ1//vn6+c9/ftx7+oTDYYVCobgDAAC36UtSrBymSDpJ+eijj3T33Xfr/PPPV35+vt555x394he/0AcffKBf/OIXKioqirt+7ty5yszMjDv27t0rSXrrrbd01lln9fucvvNvvfWWJGnlypXat2+fcnJydO655+p73/uenn322bh7/H6/ysrK9Mwzz+i9995TRUWF1q9fr3HjxmnOnDl68skndfTo0X6fV1dXp0AgEDtyc3OT/dYAAAAbJZ2kNDQ0aNGiRcrMzNSf//xnPfnkk7r88suVnp7e7/V33XWXdu7cGXeMHTs29n6iGd2UKVO0a9cuvfLKK/rOd76jDz/8ULNnz9Y111zT7/VjxozRokWL9Nprr+mpp57S1q1bdfnll2vXrl39Xl9bW6v29vbYEQwGE4oLAIAvUjQasXyYIukxKd/97nc1fPhwPfjgg/rqV7+qK664QvPnz9eFF16otLRjc56cnBydccYZ/bY1adIkvfnmm/2+13d+0qRJsXNpaWmaMWOGZsyYoUWLFunhhx/W/PnzdeONN+orX/lK3P0HDhzQ7373Oz300EPasmWLiouLVVlZqSlTpvT7vIyMDGVkZCT0PQAAwDEeWhY/6UrK2LFj9V//9V966623tGnTJqWnp+vyyy/XaaedpsWLF2v37t0Jt1VeXq63335bTz/99DHv/exnP9M//dM/6eKLLx7w/r6Eo6OjQ1LPNOVnn31W3/72t5Wdna0VK1booosu0jvvvKOmpiZVVFQMWPEBAADuYmng7AUXXKC1a9eqtbVVd9xxh3bu3KmpU6fq9ddfj13zySefqLW1Ne7oSyrKy8v1rW99S5WVlbrvvvvU0tKiP/3pT6qurtbGjRt17733asSIEZKkK6+8UnfddZe2bdumv/3tb2pubtbChQs1adIk5eXlSZKWL1+uuXPnauTIkXr++ee1Z88e3XjjjTr11FOtfEwAAFwjasOXKWxZzM3v96u8vFybNm3S3r17ddppp8Xeq6qq0pe//OW4o6GhQVLPYm+PPvqofvKTn+iuu+7S5MmTNXPmzFgS0rdGiiSVlpbq6aef1uzZszVp0iRVVlYqLy9P//3f/x1bF2X+/PlqbW3V2rVrdcEFF9jx0QAAcBmrM3vMSVJsX/Us2UGxw4cP1/XXX6/rr7/+uNctWLBACxYsOO41EyZMSChGAADgfizNCgCAQby0mBtJCgAABrE6jTilpyB7xaeZZtS22Vrhw532NNSrqytsa3tuzq7tjs3u9iIRe//Ru/vz2h2brc0NAdcHaDOvfV57ufnnqIlIUgZw4MCBf3hlz1+622oX2tKOF4XDHU6HcFz797P43+DxQx2p48CBAwoEAkP6DLp7oLFjxyoYDGrkyJHy+XwDXhcKhZSbm6tgMKisrCzLz6U9d7RFe6ndnptjoz0z/2yj0agOHDgQN3lkqJCkQGlpaRo/fnzC12dlZdnyD4L23NUW7aV2e26Ojfbc01ai7Q11BaWPl5IUW9ZJAQAAsBuVFAAATOKhvXtIUizKyMjQ0qVLbduckPbc0RbtpXZ7bo6N9tzT1lC0Z4eehe0tTEE2aLC6L2pS5xQAAB4VCoUUCAQ0a9a/a/jwEwbdztGjR7Rly6Nqb2+3ddzOUKCSAgCAQbw0cJYkBQAAg3gpSWF2DwAAcCUqKQAAGMRLlRSSFAAADMIGgwAAwJW8VElhTAoAAHAlKikAABjES5UUkhQAAEzioWXx6e4BAACuRCUFAACDRHu/rNxvCpIUAAAM4qUpyHT3AAAAV6KSAgCAQZjdAwAAXMlLSQrdPQAAwJWopAAAYBAvVVJIUgAAMIq12T2SObN7SFIAADCIlyopjEkBAACuRCUFAACTeGjvHpIUAAAMEpW1pe3NSVHo7gEAAC5FJQUAAIN4aeAsSQoAAAZhg0EAAACHUUkBAMAgXuruoZICAIBB+pIUK8dgrF69WhMmTJDf71dhYaFeffXVAa/dvXu3rrjiCk2YMEE+n0/19fWDeiZJCgAAOK4NGzaopqZGS5cu1WuvvaapU6eqtLRUH374Yb/XHzp0SBMnTtSKFSuUk5Mz6OeSpAAAYBAnKikrV67UggULVFVVpSlTpmjNmjU66aSTdP/99/d7/YwZM3THHXeovLxcGRkZg/6sJCkAABjEriQlFArFHeFwuN/ndXV1aceOHSopKYmdS0tLU0lJibZu3Tqkn5UkBQAAk0Qj1g9Jubm5CgQCsaOurq7fx+3fv1/d3d3Kzs6OO5+dna3W1tYh/ajM7gEAwIOCwaCysrJir610ywwVkhQAAAwS7f2ycr8kZWVlxSUpAxk9erSGDRumtra2uPNtbW2WBsUmgu4eAAAM8kUPnE1PT9f06dPV1NQUOxeJRNTU1KSioiK7P14cKikAAOC4ampqVFlZqfz8fBUUFKi+vl4dHR2qqqqSJFVUVGjcuHGxcS1dXV164403Yv/7vffe086dO5WZmakzzjgj4eeSpAAAYBAnVpwtKyvTvn37tGTJErW2tmratGnatGlTbDDt3r17lZb2aefM+++/r/POOy/2+s4779Sdd96p4uJiNTc3J/xcX9Sk9XEBAPCoUCikQCCgyZMLNGzY4GsM3d1HtWfPq2pvb09oTIqTGJMCAABcie4eAAAM4qUNBklSAAAwiJeSFLp7AACAK1FJAQDAIF6qpJCkAABgkqgkK4mGOTkKSQoAACaJKqKofJbuNwVjUgAAgCtRSQEAwCCMSQEAAC5lLUkxaVAK3T0AAMCVqKQAAGAQunsAAIArRaMRRaMWZvdEmd0DAABgCZUUAAAMQncPAABwJS8lKXT3AAAAV6KSAgCASaJRi3v3mFNJIUkBAMAg0d4vK/ebgiQFAACDMAUZAADAYVRSAAAwiJdm95CkAABgEC8lKXT3AAAAV6KSAgCAQbxUSSFJAQDAIF5KUujuAQAArkQlBQAAg/RUUga/1olJlRSSFAAATOKhZfHp7gEAAK5EJQUAAIOwdw8AAHAlL83uIUkBAMAgPRsMWrvfFIxJAQAArkQlBQAAg9DdAwAAXMlLSQrdPQAAwJWopAAAYBAvVVJIUgAAMIq1JEUGrZNCdw8AAHAlKikAAJjE6jonBq2TQpICAIBBepa198ay+HT3AAAAV6KSAgCAQXoGzTK7BwAAuAxJCgAAcCWrGwSywSAAAIBFVFIAADBIT2+Nle4e20IZciQpAAAYxOqYEpPGpNDdAwAAXIlKCgAABvFSJYUkBQAAk1hNMgxKUujuAQAArkQlBQAAg0QVkeSzcL85lRSSFAAADOKlMSl09wAAAFeikgIAgEG8VEkhSQEAwCAkKQAAwJW8lKQwJgUAALgSlRQAAAwSjVqcgmxQJYUkBQAAg9DdAwAA4DAqKQAAmMRDe/eQpAAAYBCry9qbtCw+3T0AAMCVqKQAAGAQZvcAAABXYnYPAACAw6ikAABgGJOqIVZQSQEAwADp6enKycmxpa2cnBylp6fb0tZQ8kW9ko4BAGC4w4cPq6ury3I76enp8vv9NkQ0tEhSAACAK9HdAwAAXIkkBQAAuBJJCgAAcCWSFAAA4EokKQAAwJVIUgAAgCuRpAAAAFf6f4UfcdGNv5gfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_show_attention(encoder, decoder,test_seq, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a425bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974520e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
